{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19aV5BjgzuYk"
      },
      "source": [
        "**OBJECT DETECTION API**\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsDUhkBZwcF4"
      },
      "outputs": [],
      "source": [
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tf_slim\n",
        "!pip install pycocotools\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models\n",
        "\n",
        "%cd /content/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "shutil.copy(\"/content/models/research/object_detection/packages/tf2/setup.py\",\"/content/models/research\")\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncO4U9tzzwQk"
      },
      "source": [
        "**DEPENDECIES**\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjjBjN0ozSXl"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.append(\"../../models/research\")\n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.protos import pipeline_pb2\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "import numpy as np\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "from imutils.video import VideoStream\n",
        "from imutils.video import FPS\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.protobuf import text_format\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt9Cf-GAzVGs"
      },
      "source": [
        "**FUNCTIONS**\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syv4vPGezUQs"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------  \n",
        "'''Funciones ejecutadas en google colaboratory: \n",
        "*create_labelmap, \n",
        "*create_tfrecords, \n",
        "*conf_pipeline, \n",
        "*train_model, \n",
        "*exporter_model, \n",
        "*process_image, \n",
        "*process_video''' \n",
        "#------------------------------------------------------  \n",
        " \n",
        "def create_labelmap(csv_path):\n",
        "    \"\"\"Funcion necesaria para la creación de los TFRecords, crea el label_map.pbtxt o en caso \n",
        "    de ya existir creará las clases del TFRecord basandose en el labelmap\n",
        "    \n",
        "    Args:\n",
        "        csv_path: Recibe la ruta al csv de train/test que será usado en la creación del label_map.pbtxt\n",
        "    Return:\n",
        "        label_map.pbtxt\"\"\"\n",
        "\n",
        "    if not os.path.exists(\"/content/label_map.pbtxt\"):\n",
        "        label_dic = pd.DataFrame(pd.read_csv(csv_path))[\"class\"].unique()\n",
        "        label_dic = dict(enumerate(label_dic,start= 1))\n",
        "        label_dic = dict(map(reversed, label_dic.items()))        \n",
        "        with open(\"/content/label_map.pbtxt\", \"w\") as f:\n",
        "            for keys, values in label_dic.items():\n",
        "                f.write('item { \\n')\n",
        "                f.write('\\tname:\\'{}\\'\\n'.format(keys))\n",
        "                f.write('\\tid:{}\\n'.format(values))\n",
        "                f.write('}\\n')\n",
        "    else:\n",
        "        label_dic= read_label_map()\n",
        "    return label_dic\n",
        "\n",
        "\n",
        "def read_label_map():\n",
        "    item_id = None\n",
        "    item_name = None\n",
        "    label_dic = {}\n",
        "    with open(\"/content/label_map.pbtxt\", \"r\") as file:\n",
        "        for line in file:\n",
        "            line.replace(\" \", \"\")\n",
        "            if line == \"item{\":\n",
        "                pass\n",
        "            elif line == \"}\":\n",
        "                pass\n",
        "            elif \"id\" in line:\n",
        "                item_id = int(line.split(\":\", 1)[1].strip())\n",
        "            elif \"name\" in line:\n",
        "                item_name = line.split(\":\")[1].replace(\"\\\"\", \" \")\n",
        "                item_name = item_name.replace(\"'\", \" \").strip()   \n",
        "\n",
        "            if item_id is not None and item_name is not None:\n",
        "                label_dic[item_name] = item_id\n",
        "                item_id = None\n",
        "                item_name = None\n",
        "    return label_dic\n",
        "\n",
        "\n",
        "#Código extraido de: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/\n",
        "#----------------------------------------------------------------------------------------------\n",
        "def class_text_to_int(row_label, labelmap):\n",
        "    if labelmap.get(row_label) != None:  \n",
        "        return labelmap[row_label]\n",
        "    else:\n",
        "        None\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    # check if the image format is matching with your images.\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class'], label_map))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "#----------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def create_tfrecords(train_csv_path, test_csv_path, compress_and_download=False):\n",
        "    \"\"\"Creación de los TFRecords de train y test a partir de las rutas de los .csv de train y\n",
        "    test, con la opción de comprimirlos junto al label_map y descargarlos\n",
        "    \n",
        "    Args:\n",
        "        train_csv_path: Recibe la ruta al archivo *train.csv producto del metodo dynotx.json_to_csv\n",
        "        test_csv_path: Recibe la ruta al archivo *test.csv producto del metodo dynotx.json_to_csv\n",
        "        compress_and_download= bool: Comprime y descarga un .zip con los archivos label_map.pbtxt train.record test.record\n",
        "    Return:\n",
        "        train.record\n",
        "        test.record\"\"\"\n",
        "                \n",
        "    if not os.path.exists(\"/content/dataset\"): \n",
        "        !unzip -q /content/dataset.zip -d /content/dataset\n",
        "\n",
        "    if os.path.exists(\"/content/label_map.pbtxt\"):\n",
        "        output_path = \"/content/train.record\"\n",
        "        path = \"/content/dataset/dataset\"    \n",
        "        \n",
        "        global label_map\n",
        "        label_map = create_labelmap(train_csv_path)\n",
        "        writer = tf.io.TFRecordWriter(output_path)\n",
        "        examples = pd.read_csv(train_csv_path)\n",
        "        grouped = split(examples, 'filename')\n",
        "        for group in grouped:\n",
        "            tf_example = create_tf_example(group, path)\n",
        "            writer.write(tf_example.SerializeToString())\n",
        "        writer.close()\n",
        "        print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "\n",
        "        output_path = \"/content/test.record\"\n",
        "        path = \"/content/dataset/dataset\"     \n",
        "        \n",
        "        label_map = create_labelmap(test_csv_path)\n",
        "        writer = tf.io.TFRecordWriter(output_path)\n",
        "        examples = pd.read_csv(test_csv_path)\n",
        "        grouped = split(examples, 'filename')\n",
        "        for group in grouped:\n",
        "            tf_example = create_tf_example(group, path)\n",
        "            writer.write(tf_example.SerializeToString())\n",
        "        writer.close()\n",
        "        print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "    else:\n",
        "        create_labelmap(train_csv_path)\n",
        "        create_tfrecords(train_csv_path, test_csv_path)\n",
        "    \n",
        "    if compress_and_download==True:\n",
        "        %cd /content\n",
        "        !zip inputs_model.zip label_map.pbtxt train.record test.record      #comprime los archivos creados por la funci�n\n",
        "        files.download(\"/content/inputs_model.zip\")\n",
        "\n",
        "\n",
        "def download_model():\n",
        "    !wget --no-check-certificate http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz \\\n",
        "        -O /content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "    !tar -zxvf /content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "    \n",
        "    output_path = os.path.join(os.getcwd(),'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8')\n",
        "    shutil.copytree(\"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\", \"/content/models/research/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\")\n",
        "    os.remove(\"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\")\n",
        "\n",
        "    if not os.path.exists(\"/content/model\"): os.mkdir(\"/content/model\") \n",
        "    if not os.path.exists(\"/content/model/pipeline.config\"): shutil.copyfile(\"{}/pipeline.config\".format(output_path), \"/content/model/pipeline.config\")\n",
        "\n",
        "\n",
        "def unzip_inputs():\n",
        "    if os.path.exists(\"/content/inputs_model.zip\"): \n",
        "        !unzip -q /content/inputs_model.zip -d /content\n",
        "\n",
        "\n",
        "def conf_pipeline(batch=8, num_steps=5000):\n",
        "    \"\"\"Función que configura el pipeline de entrenamiento.\n",
        "    \n",
        "    Parameters:\n",
        "        batch= int: Número de imagenes que consumirá el modelo por ciclo\n",
        "        num_steps= int: Número de ciclos ejecutados en el entrenamiento\n",
        "    Return: \n",
        "        pipeline.config modificado\"\"\"\n",
        "\n",
        "    global num_stps\n",
        "    num_stps=num_steps \n",
        "\n",
        "    if not os.path.exists(\"/content/label_map.pbtxt\") or not os.path.exists(\"/content/train.record\") or not os.path.exists(\"/content/test.record\"): unzip_inputs()\n",
        "\n",
        "    classes = len(read_label_map().keys())\n",
        "\n",
        "    if not os.path.exists(\"/content/models/research/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint\"): download_model()\n",
        "\n",
        "    config = config_util.get_configs_from_pipeline_file(\"/content/model/pipeline.config\")\n",
        "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "    with tf.io.gfile.GFile(\"/content/model/pipeline.config\", \"r\") as f:\n",
        "        proto_str = f.read()\n",
        "        text_format.Merge(proto_str, pipeline_config)\n",
        "\n",
        "    pipeline_config.model.ssd.num_classes = classes\n",
        "    pipeline_config.train_config.batch_size = batch\n",
        "    pipeline_config.train_config.fine_tune_checkpoint = \"/content/models/research/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
        "    pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "    pipeline_config.train_input_reader.label_map_path = \"/content/label_map.pbtxt\"\n",
        "    pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = \"/content/train.record\"\n",
        "    pipeline_config.eval_input_reader[0].label_map_path = \"/content/label_map.pbtxt\"\n",
        "    pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = \"/content/test.record\"\n",
        "\n",
        "    config_text = text_format.MessageToString(pipeline_config)\n",
        "    with tf.io.gfile.GFile(\"/content/model/pipeline.config\", \"wb\") as f:\n",
        "        f.write(config_text)\n",
        "        f.close()\n",
        "    return\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Función utilizada para entrenar el modelo siguiendo la configuración del pipeline.config, haciendo uso de los archivos \n",
        "    train.record, test.record y label_map.pbtxt.\n",
        "    \n",
        "    Parameters: \n",
        "        Any\n",
        "    Return: \n",
        "        model/: Carpeta con el modelo entrenado\"\"\"\n",
        "        \n",
        "    !python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={\"/content/model/pipeline.config\"} \\\n",
        "    --model_dir={\"/content/model\"} \\\n",
        "    --num_train_steps={num_stps}\n",
        "\n",
        "\n",
        "def exporter_model(download=False):\n",
        "    \"\"\"Función utilizada para exportar el modelo.\n",
        "    \n",
        "    Parameters: \n",
        "        Any\n",
        "    Return: \n",
        "        fnl_model/: Carpeta con el modelo exportable\n",
        "        fnl_model.zip: Archivo comprimido con el modelo\"\"\"\n",
        "\n",
        "    !python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --input_type image_tensor \\\n",
        "    --pipeline_config_path {\"/content/model/pipeline.config\"} \\\n",
        "    --trained_checkpoint_dir {\"/content/model\"} \\\n",
        "    --output_directory {\"/content/fnl_model\"}\n",
        "\n",
        "    if download==True:\n",
        "        !zip -r fnl_model.zip /fnl_model\n",
        "        files.download(\"/content/fnl_model.zip\")\n",
        "\n",
        "\n",
        "def process_image(image_path, max_boxes=20, threshold=0.8):\n",
        "    \"\"\"Función que consume una imagen y entrega la imagen con los bounding box, las clases y los score.\n",
        "    \n",
        "    Parameters:\n",
        "        image_path: Recibe la ruta a la imagen que será consumida\n",
        "        max_boxes: Recibe un int que indica el numero max de objetos a detectar\n",
        "        threshold: Recibe un float que indica el numero mínimo que tiene que tener un score para ser visualizado\n",
        "    Return:\n",
        "        Any\"\"\"\n",
        "\n",
        "    detect_fn = tf.saved_model.load(\"/content/fnl_model/saved_model\")    \n",
        "    category_index = label_map_util.create_category_index_from_labelmap(\"/content/label_map.pbtxt\")        \n",
        "\n",
        "    image_np = np.array(Image.open(image_path))\n",
        "    input_tensor = tf.convert_to_tensor(image_np)[tf.newaxis, ...]\n",
        "\n",
        "    prediction = detect_fn(input_tensor)\n",
        "    num_detections = int(prediction.pop('num_detections'))\n",
        "    detections = {key: value[0,:num_detections].numpy() for key, value in prediction.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_detections,\n",
        "        detections['detection_boxes'],\n",
        "        detections['detection_classes'],\n",
        "        detections['detection_scores'],\n",
        "        category_index,\n",
        "        max_boxes_to_draw=max_boxes,\n",
        "        min_score_thresh=threshold,\n",
        "        use_normalized_coordinates = True\n",
        "    )\n",
        "    cv2_imshow(image_np_with_detections)\n",
        "\n",
        "\n",
        "def process_video(video_path, skip_fps, threshold, function=None, option_vis=None):\n",
        "    \"\"\"Función que consume el video y entrega un nuevo video con las predicciones y los contadores, dependiendo la funcionalidad \n",
        "    seleccionada.\n",
        "    \n",
        "    Parameters:\n",
        "        video_path: Recibe la ruta al video que será consumido\n",
        "        skip_fps: Número de fps que el modelo estará en stand-by para realizar nuevas predicciones\n",
        "        threshold: Recibe un float que indica el numero mínimo que tiene que tener un score para ser visualizado\n",
        "        function = (tracker, counter)\n",
        "            tracker: La función realizará un seguimiento de los objetos que cruzan por el video\n",
        "            counter: La función contabilizará el número de objetos pertenecientes a las clases del modelo que se muestren en el video\n",
        "        option_vis = (id, label, centroid, only_centroid)\n",
        "            id: Se visualizará el bounding box, el centroide y su respectivo ID\n",
        "            label: Se visualizará el bounding box, el centroide, la clase y el score\n",
        "            centroid: Se visualizará el centroide y su ID\n",
        "            only_centroid: Se visualizará únicamente el centroide de los objetos\n",
        "    Return:\n",
        "        video_test_out.mp4: Video con las predicciones\"\"\" \n",
        "\n",
        "    detect_fn = tf.saved_model.load(\"/content/fnl_model/saved_model\") \n",
        "\n",
        "    vs = cv2.VideoCapture(video_path)\n",
        "    writer = None\n",
        "    W = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    ct = CentroidTracker(maxDisappeared= 20, maxDistance = 30)\n",
        "\n",
        "    trackers = []\n",
        "    trackableObjects = {}\n",
        "\n",
        "    if function==\"counter\": counters={id:0 for id in list(read_label_map().values())}\n",
        "\n",
        "    totalFrame = 0\n",
        "    totalDown = 0\n",
        "    totalUp = 0\n",
        "\n",
        "    fps = FPS().start()\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    writer = cv2.VideoWriter(\"/content/video_test_out.mp4\", fourcc, 20.0, (W, H), True)\n",
        "\n",
        "    while True:\n",
        "\n",
        "        ret, frame = vs.read()\n",
        "\n",
        "        if frame is None:\n",
        "            break\n",
        "        \n",
        "        rects = []\n",
        "        attributes = []\n",
        "\n",
        "        if totalFrame % skip_fps == 0:\n",
        "            status = \"Detecting\"\n",
        "            trackers = []\n",
        "            image_np = np.array(frame)\n",
        "\n",
        "            input_tensor = tf.convert_to_tensor(image_np)[tf.newaxis, ...]\n",
        "            detections = detect_fn(input_tensor)\n",
        "\n",
        "            detection_scores = np.array(detections[\"detection_scores\"][0])\n",
        "            detection_clean = [x for x in detection_scores if x >= threshold]\n",
        "            \n",
        "            for x in range(len(detection_clean)):\n",
        "                idx = int(detections['detection_classes'][0][x])\n",
        "                ymin, xmin, ymax, xmax = np.array(detections['detection_boxes'][0][x])\n",
        "                classes = int(np.array(detections['detection_classes'][0][x]))\n",
        "                score = detection_clean[x]\n",
        "                attribute = [classes, score]\n",
        "                box = [xmin, ymin, xmax, ymax] * np.array([W, H, W, H])\n",
        "\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "                tracker = dlib.correlation_tracker()\n",
        "                rect = dlib.rectangle(startX, startY, endX, endY)\n",
        "                tracker.start_track(frame, rect)\n",
        "\n",
        "                trackers.append(tracker)\n",
        "                attributes.append(attribute)\n",
        "\n",
        "        else:\n",
        "            for tracker in trackers:\n",
        "                status = \"Watching\"\n",
        "                tracker.update(frame)\n",
        "                pos = tracker.get_position()\n",
        "\n",
        "                startX = int(pos.left())\n",
        "                startY = int(pos.top())\n",
        "                endX = int(pos.right())\n",
        "                endY = int(pos.bottom())\n",
        "\n",
        "                rects.append((startX, startY, endX, endY))\n",
        "\n",
        "                if option_vis==\"label\" or option_vis==\"id\": cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
        "\n",
        "        objects = ct.update(rects)\n",
        "\n",
        "        for (objectID, centroid) in objects.items():\n",
        "            to = trackableObjects.get(objectID, None)\n",
        "            if to is None:\n",
        "                to = TrackableObject(objectID, centroid)\n",
        "\n",
        "            else:\n",
        "                y = [c[1] for c in to.centroids]\n",
        "                direction = centroid[1] - np.mean(y)\n",
        "                to.centroids.append(centroid)\n",
        "                \n",
        "                if bool(to.attributes)==False and bool(attributes):\n",
        "                    to.attributes = attributes\n",
        "\n",
        "                    if not to.counted:\n",
        "                        if function==\"counter\":\n",
        "                            counters[to.attributes[0][0]] += 1\n",
        "                            to.counted = True\n",
        "\n",
        "                if not to.counted:\n",
        "                    if function==\"tracker\" or function==None:\n",
        "                        if direction < 0 and centroid[1] < H//2:\n",
        "                            totalUp += 1\n",
        "                            to.counted = True\n",
        "                        elif direction > 0 and centroid[1] > H//2:\n",
        "                            totalDown += 1\n",
        "                            to.counted = True\n",
        "                        \n",
        "\n",
        "            trackableObjects[objectID] = to\n",
        "\n",
        "            if function==\"tracker\" or function==None: cv2.line(frame, (0, H//2), (W, H//2), (0,0,255), 2)\n",
        "                            \n",
        "            if bool(to.attributes):\n",
        "                if option_vis==\"id\" or option_vis==None:\n",
        "                    text = \"ID {}\".format(objectID)\n",
        "                    cv2.putText(frame, text, (centroid[0]-20, centroid[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "                    cv2.circle(frame, (centroid[0], centroid[1]), 4, (0,0,255), -1)\n",
        "                if option_vis==\"label\":\n",
        "                    text = \"{0} at {1:.2f}%\".format(list(read_label_map().keys())[to.attributes[0][0]-1],(float(to.attributes[0][1])*100))\n",
        "                    cv2.putText(frame, text, (centroid[0]-50, centroid[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "                    cv2.circle(frame, (centroid[0], centroid[1]), 4, (0,0,255), -1)\n",
        "                if option_vis==\"centroid\":\n",
        "                    text = \"ID {}\".format(objectID)\n",
        "                    cv2.putText(frame, text, (centroid[0]-20, centroid[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "                    cv2.circle(frame, (centroid[0], centroid[1]), 4, (0,0,255), -1)\n",
        "                if option_vis==\"only_centroid\":\n",
        "                    cv2.circle(frame, (centroid[0], centroid[1]), 4, (0,0,255), -1)        \n",
        "    \n",
        "\n",
        "        if function==\"tracker\" or function==None:\n",
        "            info = [(\"Subiendo\", totalUp), (\"Bajando\", totalDown), (\"Estado\", status)]\n",
        "            cv2.rectangle(frame, (5, H - ((len(info)*20) + 20)), ( int(W*0.25) , H - 10), (0, 0, 0), -1)\n",
        "\n",
        "        if function==\"counter\":\n",
        "            info = [(\"Labels: \", {name:cont for (name,cont) in zip(read_label_map().keys(),list(counters.values()))}), (\"Estado\", status)]\n",
        "            cv2.rectangle(frame, (5, H - ((len(info)*20) + 20)), ((len(info[0][1])*125), H - 10), (0, 0, 0), -1)\n",
        "\n",
        "        for (i, (k,v)) in enumerate(info):\n",
        "            text = \"{}: {}\".format(k,v)\n",
        "            cv2.putText(frame, text, (10, H - ((i*20) + 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
        "\n",
        "        writer.write(frame)\n",
        "        totalFrame += 1\n",
        "        fps.update()\n",
        "\n",
        "    fps.stop()\n",
        "\n",
        "    print(\"Tiempo completo {}\".format(fps.elapsed()))\n",
        "    print(\"Tiempo aproximado por frame {}\".format(fps.fps()))\n",
        "\n",
        "    writer.release()\n",
        "    vs.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPDaFcij0VhE"
      },
      "source": [
        "**EXECUTION**\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUZqVNE90Uqz"
      },
      "outputs": [],
      "source": [
        "# Luego de utilizar el labelStudio para etiquetar nuestro dataset, procedemos con la creación de los inputs del modelo.\n",
        "\n",
        "# Cargar en la caarpeta /content los archivos: \n",
        "#   centroidtracker.py, trackableobject.py = Estos se encuentra en el repositorio\n",
        "#   labelstudio_train.csv, labelstudio_test.csv, dataset.zip = Son los pertenecientes a cada proyecto\n",
        "\n",
        "create_labelmap(\"/content/labelStudio_train.csv\")\n",
        "create_tfrecords(\"/content/labelStudio_train.csv\", \"/content/labelStudio_test.csv\", True)\n",
        "\n",
        "#Una vez obtenidos el labelmap y los .records de entrenamiento y test, modificamos el pipeline y entrenamos el modelo\n",
        "conf_pipeline(32, 4500) \n",
        "train_model()\n",
        "exporter_model(True)\n",
        "\n",
        "#Testeamos el modelo tanto para imágenes como para videos\n",
        "process_image(\"/content/enfermo1.jpeg\")\n",
        "process_video(\"/content/test_video5.mp4\", 30, 0.8, function=\"counter\", option_vis=\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDp74c8rzppz"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/model\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e08962900da9caa813cd73b2663927d4048c3687cb532f5adf355ed691b87ee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
